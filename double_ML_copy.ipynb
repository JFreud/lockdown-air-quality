{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import sklearn\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = pd.read_csv(\"data/wf.csv\")\n",
    "city_yb = pd.read_csv(\"data/city_yb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = pd.read_csv(\"data/wf.csv\")\n",
    "city_yb = pd.read_csv(\"data/city_yb.csv\")\n",
    "\n",
    "wf[\"temp2\"] = wf[\"temp\"] ** 2\n",
    "wf[\"l_aqi\"] = np.log(1 + wf[\"aqi\"])\n",
    "wf[\"l_pm\"] = np.log(1 + wf[\"pm\"])\n",
    "wf2020 = wf[(wf[\"daynum\"] >= 8401) & (wf[\"daynum\"]<= 8461)].dropna(\n",
    "    subset = ['aqi', 'pm']\n",
    ")\n",
    "wf2020['cities'] = wf2020['city_code'].astype('category')\n",
    "wf2020['days'] = wf2020['daynum'].astype('category')\n",
    "wf2020 = pd.get_dummies(wf2020, drop_first=True)\n",
    "\n",
    "fixed = ['treat']\n",
    "for col in wf2020.columns:\n",
    "    if 'cities' in col or 'days' in col:\n",
    "        fixed.append(col)\n",
    "        \n",
    "weather = ['prec', 'snow', 'temp', 'temp2']\n",
    "out = [\"aqi\", \"l_aqi\", \"pm\", \"l_pm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOUBLE ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = wf2020[wf2020['treat'] == 1]\n",
    "treated = treated[['daynum', 'city_code']].groupby('city_code')\n",
    "first = treated.apply(lambda x: x.sort_values(by = 'daynum', ascending=True).head(1))\n",
    "\n",
    "day, count = np.unique(first.daynum, return_counts = True)\n",
    "treat_day = day[count == max(count)][0]\n",
    "\n",
    "num_cities = {d:c for d,c in zip(day, count)}\n",
    "first = {city:day for day, city in first.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf2020 = wf2020.assign(first = [first.get(city, 0) for city in wf2020['city_code']])\n",
    "group = wf2020[(wf2020['first'] == treat_day) | (wf2020['first'] == 0)]\n",
    "wf2020['first'] = wf2020['first'].astype('category')\n",
    "wf2020 = pd.get_dummies(wf2020, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "group['pre'] = group['daynum'] < treat_day\n",
    "group = group.groupby(['city_code', 'pre']).mean().reset_index('pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jtf/anaconda3/envs/mamba-base-env/envs/nn_class/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "compact = group[~group['pre']]\n",
    "aqi = group['aqi'].values\n",
    "compact['Y1-Y0'] = aqi[~group['pre']] - aqi[group['pre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact = compact.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format this in a manner sympatico with ATT estimation\n",
    "\n",
    "outcome = compact['Y1-Y0']\n",
    "treatment = compact['treat']\n",
    "confounders = compact[weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of fit model 592.0016742435274\n",
      "Test MSE of no-covariate model 763.2330734709204\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the conditional expected outcome\n",
    "\n",
    "# TODO(victorveitch) the covariates have basically no predictive power, replace this example with something better\n",
    "\n",
    "# make a function that returns a sklearn model for later use in k-folding\n",
    "def make_Q_model():\n",
    "    return LinearRegression()\n",
    "    return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=100, max_depth=5)\n",
    "Q_model = make_Q_model()\n",
    "\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_w_treatment = confounders.copy()\n",
    "X_w_treatment[\"treatment\"] = treatment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w_treatment, outcome, test_size=0.2)\n",
    "Q_model.fit(X_train, y_train)\n",
    "y_pred = Q_model.predict(X_test)\n",
    "\n",
    "test_mse=mean_squared_error(y_pred, y_test)\n",
    "print(f\"Test MSE of fit model {test_mse}\") \n",
    "baseline_mse=mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
    "print(f\"Test MSE of no-covariate model {baseline_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CE of fit model 0.2074859981907707\n",
      "Test CE of no-covariate model 0.23372561765023045\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the propensity score\n",
    "\n",
    "def make_g_model():\n",
    "    return LogisticRegression(max_iter=1000)\n",
    "    return RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=100, max_depth=5)\n",
    "\n",
    "g_model = make_g_model()\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_train, X_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.2)\n",
    "g_model.fit(X_train, a_train)\n",
    "a_pred = g_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "test_ce=log_loss(a_test, a_pred)\n",
    "print(f\"Test CE of fit model {test_ce}\") \n",
    "baseline_ce=log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "print(f\"Test CE of no-covariate model {baseline_ce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to implement the cross fitting\n",
    "\n",
    "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the treatment A. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns an array containing the predictions  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (which implements fit and predict_prob)\n",
    "    X: dataframe of variables to adjust for\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    \"\"\"\n",
    "    predictions = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, A):\n",
    "        X_train = X.loc[train_index]\n",
    "        A_train = A.loc[train_index]\n",
    "        g = make_model()\n",
    "        g.fit(X_train, A_train)\n",
    "\n",
    "        # get predictions for split\n",
    "        predictions[test_index] = g.predict_proba(X.loc[test_index])[:, 1]\n",
    "\n",
    "    assert np.isnan(predictions).sum() == 0\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def outcome_k_fold_fit_and_predict(make_model, X:pd.DataFrame, y:np.array, A:np.array, n_splits:int, output_type:str):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the outcome Y. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns two arrays containing the predictions for all units untreated, all units treated  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (that implements fit and either predict_prob or predict)\n",
    "    X: dataframe of variables to adjust for\n",
    "    y: array of outcomes\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    output_type: type of outcome, \"binary\" or \"continuous\"\n",
    "\n",
    "    \"\"\"\n",
    "    predictions0 = np.full_like(A, np.nan, dtype=float)\n",
    "    predictions1 = np.full_like(y, np.nan, dtype=float)\n",
    "    if output_type == 'binary':\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    elif output_type == 'continuous':\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    # include the treatment as input feature\n",
    "    X_w_treatment = X.copy()\n",
    "    X_w_treatment[\"A\"] = A\n",
    "\n",
    "    # for predicting effect under treatment / control status for each data point \n",
    "    X0 = X_w_treatment.copy()\n",
    "    X0[\"A\"] = 0\n",
    "    X1 = X_w_treatment.copy()\n",
    "    X1[\"A\"] = 1\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_w_treatment, y):\n",
    "        X_train = X_w_treatment.loc[train_index]\n",
    "        y_train = y.loc[train_index]\n",
    "        q = make_model()\n",
    "        q.fit(X_train, y_train)\n",
    "\n",
    "        if output_type =='binary':\n",
    "            predictions0[test_index] = q.predict_proba(X0.loc[test_index])[:, 1]\n",
    "            predictions1[test_index] = q.predict_proba(X1.loc[test_index])[:, 1]\n",
    "        elif output_type == 'continuous':\n",
    "            predictions0[test_index] = q.predict(X0.loc[test_index])\n",
    "            predictions1[test_index] = q.predict(X1.loc[test_index])\n",
    "\n",
    "    assert np.isnan(predictions0).sum() == 0\n",
    "    assert np.isnan(predictions1).sum() == 0\n",
    "    return predictions0, predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = treatment_k_fold_fit_and_predict(make_g_model, X=confounders, A=treatment, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q0,Q1=outcome_k_fold_fit_and_predict(make_Q_model, X=confounders, y=outcome, A=treatment, n_splits=10, output_type=\"continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "      <th>A</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.135846</td>\n",
       "      <td>-24.575467</td>\n",
       "      <td>-41.087033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-47.483196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.094061</td>\n",
       "      <td>-17.032831</td>\n",
       "      <td>-34.925517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-38.480972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100673</td>\n",
       "      <td>-15.995116</td>\n",
       "      <td>-32.506683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-67.323789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094208</td>\n",
       "      <td>-15.429095</td>\n",
       "      <td>-34.433198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.060072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.091899</td>\n",
       "      <td>-25.442115</td>\n",
       "      <td>-43.514097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.856384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          g         Q0         Q1    A          Y\n",
       "0  0.135846 -24.575467 -41.087033  1.0 -47.483196\n",
       "1  0.094061 -17.032831 -34.925517  0.0 -38.480972\n",
       "2  0.100673 -15.995116 -32.506683  0.0 -67.323789\n",
       "3  0.094208 -15.429095 -34.433198  0.0 -37.060072\n",
       "4  0.091899 -25.442115 -43.514097  0.0  -1.856384"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_and_nuisance_estimates = pd.DataFrame({'g': g, 'Q0': Q0, 'Q1': Q1, 'A': treatment, 'Y': outcome})\n",
    "data_and_nuisance_estimates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "    \"\"\"\n",
    "    # Double ML estimator for the ATT\n",
    "    This uses the ATT specific scores, see equation 3.9 of https://www.econstor.eu/bitstream/10419/149795/1/869216953.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    if prob_t is None:\n",
    "        prob_t = A.mean() # estimate marginal probability of treatment\n",
    "\n",
    "    tau_hat = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0)).mean()/ prob_t\n",
    "  \n",
    "    scores = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0) - tau_hat*A) / prob_t\n",
    "    n = Y.shape[0] # number of observations\n",
    "    std_hat = np.std(scores) / np.sqrt(n)\n",
    "\n",
    "    return tau_hat, std_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate is -19.319572199045226 pm 11.349616379309929\n"
     ]
    }
   ],
   "source": [
    "tau_hat, std_hat = att_aiptw(**data_and_nuisance_estimates)\n",
    "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18.886275852614084"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for comparison, the point estimate without any covariate correction\n",
    "outcome[treatment==1].mean()-outcome[treatment==0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf2020 = wf2020.assign(first = [first.get(city, 0) for city in wf2020['city_code']])\n",
    "wf2020['A'] = (wf2020['daynum'] == wf2020['first']).astype('int64')\n",
    "\n",
    "wf2020['diff'] = wf2020.sort_values(by = 'daynum')['aqi'] \\\n",
    "            - wf2020.sort_values(by = 'daynum').groupby('city_code')['aqi'].shift(1)\n",
    "wf2020 = wf2020.dropna(subset= ['diff'])\n",
    "\n",
    "outcome = wf2020['diff']\n",
    "treatment = wf2020['A']\n",
    "confounders = wf2020[['first'] + weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of fit model 1090.0377218848553\n",
      "Test MSE of no-covariate model 1165.2400531127032\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the conditional expected outcome\n",
    "\n",
    "# TODO(victorveitch) the covariates have basically no predictive power, replace this example with something better\n",
    "\n",
    "# make a function that returns a sklearn model for later use in k-folding\n",
    "def make_Q_model():\n",
    "    #return LinearRegression()\n",
    "    return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators = 100, max_depth=10)\n",
    "Q_model = make_Q_model()\n",
    "\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_w_treatment = confounders.copy()\n",
    "X_w_treatment[\"treatment\"] = treatment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w_treatment, outcome, test_size=0.2)\n",
    "Q_model.fit(X_train, y_train)\n",
    "y_pred = Q_model.predict(X_test)\n",
    "\n",
    "test_mse=mean_squared_error(y_pred, y_test)\n",
    "print(f\"Test MSE of fit model {test_mse}\") \n",
    "baseline_mse=mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
    "print(f\"Test MSE of no-covariate model {baseline_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CE of fit model 0.018717674730204158\n",
      "Test CE of no-covariate model 0.024077153205500912\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the propensity score\n",
    "\n",
    "def make_g_model():\n",
    "    #return LogisticRegression(max_iter=1000)\n",
    "    return RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=100, max_depth=3)\n",
    "\n",
    "g_model = make_g_model()\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_train, X_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.2)\n",
    "g_model.fit(X_train, a_train)\n",
    "a_pred = g_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "test_ce=log_loss(a_test, a_pred)\n",
    "print(f\"Test CE of fit model {test_ce}\") \n",
    "baseline_ce=log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "print(f\"Test CE of no-covariate model {baseline_ce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00043236041379117717"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_pred.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "    \"\"\"\n",
    "    # Double ML estimator for the ATT\n",
    "    This uses the ATT specific scores, see equation 3.9 of https://www.econstor.eu/bitstream/10419/149795/1/869216953.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    if prob_t is None:\n",
    "        prob_t = A.mean() # estimate marginal probability of treatment\n",
    "\n",
    "    tau_hat = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0)).mean()/ prob_t\n",
    "  \n",
    "    scores = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0) - tau_hat*A) / prob_t\n",
    "    n = Y.shape[0] # number of observations\n",
    "    std_hat = np.std(scores) / np.sqrt(n)\n",
    "\n",
    "    return tau_hat, std_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.684548868980686"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for comparison, the point estimate without any covariate correction\n",
    "outcome[treatment==1].mean()-outcome[treatment==0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=3, random_state=42)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = dict()\n",
    "Q_model.fit(X_w_treatment, outcome)\n",
    "g_model.fit(confounders, treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.916 pm 1.542\n"
     ]
    }
   ],
   "source": [
    "for day in np.unique(wf2020['daynum']):\n",
    "    df = wf2020[wf2020['daynum'] == day]\n",
    "    outcome_t = df['diff']\n",
    "    treatment_t = df['A']\n",
    "    confounders_t = df[['first'] + weather]\n",
    "    \n",
    "    if df['A'].sum() == 0 or num_cities[day] < 2:\n",
    "        continue\n",
    "    \n",
    "    X1 = confounders_t.copy()\n",
    "    X0 = confounders_t.copy()\n",
    "    X1[\"treatment\"] = 1\n",
    "    X0[\"treatment\"] = 0\n",
    "    \n",
    "    Q0 = Q_model.predict(X0)\n",
    "    Q1 = Q_model.predict(X1)\n",
    "    g = g_model.predict_proba(confounders_t)[:,1]\n",
    "    \n",
    "    est, sd = att_aiptw(Q0, Q1, g, treatment_t, outcome_t)\n",
    "    res[day] = (est, sd)\n",
    "    \n",
    "inv_var = np.array([1/v**2 for p,v in res.values()])\n",
    "point = np.array([p for p,v in res.values()])    \n",
    "\n",
    "tau_hat = (point * inv_var).sum()/inv_var.sum()\n",
    "std_hat = np.sqrt(1/inv_var.sum())\n",
    "\n",
    "print('%0.3f pm %0.3f' % (tau_hat, std_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_var = 'aqi'\n",
    "pd.options.mode.chained_assignment = None  # default='warn' # NOTE: is this a bad?\n",
    "# trends across time periods (days)\n",
    "time_periods = np.sort(np.unique(wf2020[\"daynum\"]))\n",
    "pairwise_time_diffs = []\n",
    "\n",
    "for t in range(0, len(time_periods)-1):\n",
    "    wf_pt_post = wf2020[wf2020[\"daynum\"] == time_periods[t+1]]\n",
    "    outcome_t = wf_pt_post['diff']\n",
    "    treatment_t = wf_pt_post['A']\n",
    "    confounders_t = wf_pt_post[['first'] + weather]\n",
    "    X1 = confounders_t.copy()\n",
    "    X0 = confounders_t.copy()\n",
    "    X1[\"treatment\"] = 1\n",
    "    X0[\"treatment\"] = 0\n",
    "    \n",
    "    Q0 = Q_model.predict(X0)\n",
    "    Q1 = Q_model.predict(X1)\n",
    "    print(time_periods[t+1], np.mean(Q1 - Q0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create week coefficient \n",
    "wf2020[\"week_coef\"] = np.floor((wf2020[\"daynum\"] - wf2020[\"first\"])/7).astype(int)\n",
    "# set -1 lead and untreated to NaN so they don't get week0 dummy\n",
    "wf2020[\"week_coef\"] = np.where((wf2020[\"week_coef\"] == -1), np.NaN, wf2020[\"week_coef\"])\n",
    "wf2020[\"week_coef\"][wf2020[\"first\"] == 0] = np.NaN\n",
    "wf2020[\"week_coef\"] = wf2020[\"week_coef\"].astype('category')\n",
    "wf2020 = pd.get_dummies(wf2020)\n",
    "\n",
    "week_coef = []\n",
    "for col in wf2020.columns:\n",
    "    if 'week_coef' in col:\n",
    "        week_coef.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_coef_-7.0 -0.3364145862151906\n",
      "week_coef_-6.0 -2.93730435177273\n",
      "week_coef_-5.0 -1.2571279636109294\n",
      "week_coef_-4.0 -2.7465710435608894\n",
      "week_coef_-3.0 -1.7802101708449336\n",
      "week_coef_-2.0 -2.670474698963522\n",
      "week_coef_0.0 -2.3305858196633773\n",
      "week_coef_1.0 -1.6426142976755265\n",
      "week_coef_2.0 -0.9044038526144369\n",
      "week_coef_3.0 -1.50629645110028\n",
      "week_coef_4.0 -0.8348632231018416\n",
      "week_coef_5.0 -0.6930900445352839\n"
     ]
    }
   ],
   "source": [
    "pairwise_time_diffs = []\n",
    "\n",
    "for w in range(0, len(week_coef)-1):\n",
    "    wf_pt_pre = wf2020[wf2020[week_coef[w]] == 1 ]\n",
    "    wf_pt_post = wf2020[wf2020[week_coef[w+1]] == 1]\n",
    "    outcome_t = wf_pt_post['diff']\n",
    "    treatment_t = wf_pt_post['A']\n",
    "    confounders_t = wf_pt_post[['first'] + weather]\n",
    "    X1 = confounders_t.copy()\n",
    "    X0 = confounders_t.copy()\n",
    "    X1[\"treatment\"] = 1\n",
    "    X0[\"treatment\"] = 0\n",
    "    \n",
    "    Q0 = Q_model.predict(X0)\n",
    "    Q1 = Q_model.predict(X1)\n",
    "    print(week_coef[w+1], np.mean(Q1) - np.mean(Q0))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_coef_-8.0</th>\n",
       "      <th>week_coef_-7.0</th>\n",
       "      <th>week_coef_-6.0</th>\n",
       "      <th>week_coef_-5.0</th>\n",
       "      <th>week_coef_-4.0</th>\n",
       "      <th>week_coef_-3.0</th>\n",
       "      <th>week_coef_-2.0</th>\n",
       "      <th>week_coef_0.0</th>\n",
       "      <th>week_coef_1.0</th>\n",
       "      <th>week_coef_2.0</th>\n",
       "      <th>week_coef_3.0</th>\n",
       "      <th>week_coef_4.0</th>\n",
       "      <th>week_coef_5.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144413</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144414</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144415</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144416</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144417</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19440 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        week_coef_-8.0  week_coef_-7.0  week_coef_-6.0  week_coef_-5.0  \\\n",
       "366                  0               0               1               0   \n",
       "367                  0               0               1               0   \n",
       "368                  0               0               1               0   \n",
       "369                  0               0               1               0   \n",
       "370                  0               0               0               1   \n",
       "...                ...             ...             ...             ...   \n",
       "144413               0               0               0               0   \n",
       "144414               0               0               0               0   \n",
       "144415               0               0               0               0   \n",
       "144416               0               0               0               0   \n",
       "144417               0               0               0               0   \n",
       "\n",
       "        week_coef_-4.0  week_coef_-3.0  week_coef_-2.0  week_coef_0.0  \\\n",
       "366                  0               0               0              0   \n",
       "367                  0               0               0              0   \n",
       "368                  0               0               0              0   \n",
       "369                  0               0               0              0   \n",
       "370                  0               0               0              0   \n",
       "...                ...             ...             ...            ...   \n",
       "144413               0               0               0              0   \n",
       "144414               0               0               0              0   \n",
       "144415               0               0               0              0   \n",
       "144416               0               0               0              0   \n",
       "144417               0               0               0              0   \n",
       "\n",
       "        week_coef_1.0  week_coef_2.0  week_coef_3.0  week_coef_4.0  \\\n",
       "366                 0              0              0              0   \n",
       "367                 0              0              0              0   \n",
       "368                 0              0              0              0   \n",
       "369                 0              0              0              0   \n",
       "370                 0              0              0              0   \n",
       "...               ...            ...            ...            ...   \n",
       "144413              0              0              0              0   \n",
       "144414              0              0              0              0   \n",
       "144415              0              0              0              0   \n",
       "144416              0              0              0              0   \n",
       "144417              0              0              0              0   \n",
       "\n",
       "        week_coef_5.0  \n",
       "366                 0  \n",
       "367                 0  \n",
       "368                 0  \n",
       "369                 0  \n",
       "370                 0  \n",
       "...               ...  \n",
       "144413              0  \n",
       "144414              0  \n",
       "144415              0  \n",
       "144416              0  \n",
       "144417              0  \n",
       "\n",
       "[19440 rows x 13 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf2020[week_coef]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  8401 -> 8402 \t 0.0\n",
      "t:  8402 -> 8403 \t 0.0\n",
      "t:  8403 -> 8404 \t 0.0\n",
      "t:  8404 -> 8405 \t 0.0\n",
      "t:  8405 -> 8406 \t 0.0\n",
      "t:  8406 -> 8407 \t 0.0\n",
      "t:  8407 -> 8408 \t 0.0\n",
      "t:  8408 -> 8409 \t 0.0\n",
      "t:  8409 -> 8410 \t 0.0\n",
      "t:  8410 -> 8411 \t 0.0\n",
      "t:  8411 -> 8412 \t 0.0\n",
      "t:  8412 -> 8413 \t 0.0\n",
      "t:  8413 -> 8414 \t 0.0\n",
      "t:  8414 -> 8415 \t 0.0\n",
      "t:  8415 -> 8416 \t 0.0\n",
      "t:  8416 -> 8417 \t 0.0\n",
      "t:  8417 -> 8418 \t 0.0\n",
      "t:  8418 -> 8419 \t 0.0\n",
      "t:  8419 -> 8420 \t 0.0\n",
      "t:  8420 -> 8421 \t 0.0\n",
      "t:  8421 -> 8422 \t 0.0\n",
      "t:  8422 -> 8423 \t 0.0\n",
      "t:  8423 -> 8424 \t 0.0\n",
      "t:  8424 -> 8425 \t 0.0\n",
      "t:  8425 -> 8426 \t 0.0\n",
      "t:  8426 -> 8427 \t 0.0\n",
      "t:  8427 -> 8428 \t 0.0\n",
      "t:  8428 -> 8429 \t 0.0\n",
      "t:  8429 -> 8430 \t 0.0\n",
      "t:  8430 -> 8431 \t 0.0\n",
      "t:  8431 -> 8432 \t 0.0\n",
      "t:  8432 -> 8433 \t 0.0\n",
      "t:  8433 -> 8434 \t 0.0\n",
      "t:  8434 -> 8435 \t 0.0\n",
      "t:  8435 -> 8436 \t -10.130868148113361\n",
      "t:  8436 -> 8437 \t 12.819396864298037\n",
      "t:  8437 -> 8438 \t 6.713918999099076\n",
      "t:  8438 -> 8439 \t 6.396595591412179\n",
      "t:  8439 -> 8440 \t 22.88629774082264\n",
      "t:  8440 -> 8441 \t 19.856048132663567\n",
      "t:  8441 -> 8442 \t -31.635567965823757\n",
      "t:  8442 -> 8443 \t -26.72503158309498\n",
      "t:  8443 -> 8444 \t -4.168769278777265\n",
      "t:  8444 -> 8445 \t -7.895598925247872\n",
      "t:  8445 -> 8446 \t 9.030092665716792\n",
      "t:  8446 -> 8447 \t 10.358085655419053\n",
      "t:  8447 -> 8448 \t -16.64977014463289\n",
      "t:  8448 -> 8449 \t 1.833754574081749\n",
      "t:  8449 -> 8450 \t 8.254021862468274\n",
      "t:  8450 -> 8451 \t 4.448311205494339\n",
      "t:  8451 -> 8452 \t 3.453802207288433\n",
      "t:  8452 -> 8453 \t -19.23159012203717\n",
      "t:  8453 -> 8454 \t 0.18043939196151434\n",
      "t:  8454 -> 8455 \t -2.7435712099854466\n",
      "t:  8455 -> 8456 \t -16.10351532590872\n",
      "t:  8456 -> 8457 \t 11.82178132107575\n",
      "t:  8457 -> 8458 \t 11.091042284554575\n",
      "t:  8458 -> 8459 \t 12.49691603202189\n",
      "t:  8459 -> 8460 \t -4.0672358781990905\n",
      "t:  8460 -> 8461 \t -11.001037674749565\n"
     ]
    }
   ],
   "source": [
    "# check parallel trends \n",
    "# E[Y_{t+1} - Y_{t} | A=1, X] - E[Y_{t+1} - Y_{t} | A=0, X] = 0 for pre-treatment periods\n",
    "\n",
    "wf_pt = wf[(wf[\"daynum\"] >= 8401) & (wf[\"daynum\"]<= 8461)].dropna(\n",
    "    subset = ['aqi', 'pm']\n",
    ")\n",
    "wf_pt['cities'] = wf_pt['city_code'].astype('category')\n",
    "wf_pt['days'] = wf_pt['daynum'].astype('category')\n",
    "wf_pt = pd.get_dummies(wf_pt, drop_first=True)\n",
    "\n",
    "wf_pt = wf_pt.assign(first = [first.get(city, 0) for city in wf_pt['city_code']])\n",
    "wf_pt = wf_pt[(wf_pt['first'] == treat_day) | (wf_pt['first'] == 0)]\n",
    "# wf_pt['first'] = wf_pt['first'].astype('category')\n",
    "#wf_pt = pd.get_dummies(wf_pt, drop_first=True)\n",
    "\n",
    "\n",
    "out_var = 'aqi'\n",
    "pd.options.mode.chained_assignment = None  # default='warn' # NOTE: is this a bad?\n",
    "# trends across time periods (days)\n",
    "time_periods = np.sort(np.unique(wf_pt[\"daynum\"]))\n",
    "pairwise_time_diffs = []\n",
    "for t in range(0, len(time_periods)-1):\n",
    "    wf_pt_pre = wf_pt[wf_pt[\"daynum\"] == time_periods[t]]\n",
    "    wf_pt_post = wf_pt[wf_pt[\"daynum\"] == time_periods[t+1]]\n",
    "    wf_pt_post['Y1-Y0'] = wf_pt_post[out_var].values - wf_pt_pre[out_var].values\n",
    "    wf_pt_post.reset_index(inplace=True)\n",
    "    \n",
    "    outcome_pt = wf_pt_post['Y1-Y0']\n",
    "    treatment_pt = wf_pt_post['treat']\n",
    "    confounders_pt = wf_pt_post[weather]\n",
    "    # NOTE: this doesn't make sense bc the coefficient will always be 0\n",
    "    Q0_pt,Q1_pt=outcome_k_fold_fit_and_predict(make_Q_model, X=confounders_pt, y=outcome_pt, A=treatment_pt, n_splits=10, output_type=\"continuous\")\n",
    "    print(\"t: \", time_periods[t], \"->\", time_periods[t+1], \"\\t\", np.mean(Q1_pt - Q0_pt))\n",
    "    pairwise_time_diffs.append(np.mean(Q1_pt - Q0_pt))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a525e117b74db75a0938b9a7ba442b7bcb3e21c3f177debe4c065c7a7c1b83e"
  },
  "kernelspec": {
   "display_name": "nn_class",
   "language": "python",
   "name": "nn_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
