{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credibility/Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from austen_plots.AustenPlot import AustenPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "def make_Q_model():\n",
    "    return LinearRegression()\n",
    "    return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=100, max_depth=5)\n",
    "\n",
    "def make_g_model():\n",
    "    return LogisticRegression(max_iter=1000)\n",
    "    return RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=100, max_depth=5)\n",
    "\n",
    "# helper functions to implement the cross fitting\n",
    "\n",
    "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the treatment A. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns an array containing the predictions  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (which implements fit and predict_prob)\n",
    "    X: dataframe of variables to adjust for\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    \"\"\"\n",
    "    predictions = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, A):\n",
    "        X_train = X.loc[train_index]\n",
    "        A_train = A.loc[train_index]\n",
    "        g = make_model()\n",
    "        g.fit(X_train, A_train)\n",
    "\n",
    "        # get predictions for split\n",
    "        predictions[test_index] = g.predict_proba(X.loc[test_index])[:, 1]\n",
    "\n",
    "    assert np.isnan(predictions).sum() == 0\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def outcome_k_fold_fit_and_predict(make_model, X:pd.DataFrame, y:np.array, A:np.array, n_splits:int, output_type:str):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the outcome Y. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns two arrays containing the predictions for all units untreated, all units treated  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (that implements fit and either predict_prob or predict)\n",
    "    X: dataframe of variables to adjust for\n",
    "    y: array of outcomes\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    output_type: type of outcome, \"binary\" or \"continuous\"\n",
    "\n",
    "    \"\"\"\n",
    "    predictions0 = np.full_like(A, np.nan, dtype=float)\n",
    "    predictions1 = np.full_like(y, np.nan, dtype=float)\n",
    "    if output_type == 'binary':\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    elif output_type == 'continuous':\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    # include the treatment as input feature\n",
    "    X_w_treatment = X.copy()\n",
    "    X_w_treatment[\"A\"] = A\n",
    "\n",
    "    # for predicting effect under treatment / control status for each data point \n",
    "    X0 = X_w_treatment.copy()\n",
    "    X0[\"A\"] = 0\n",
    "    X1 = X_w_treatment.copy()\n",
    "    X1[\"A\"] = 1\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_w_treatment, y):\n",
    "        X_train = X_w_treatment.loc[train_index]\n",
    "        y_train = y.loc[train_index]\n",
    "        q = make_model()\n",
    "        q.fit(X_train, y_train)\n",
    "\n",
    "        if output_type =='binary':\n",
    "            predictions0[test_index] = q.predict_proba(X0.loc[test_index])[:, 1]\n",
    "            predictions1[test_index] = q.predict_proba(X1.loc[test_index])[:, 1]\n",
    "        elif output_type == 'continuous':\n",
    "            predictions0[test_index] = q.predict(X0.loc[test_index])\n",
    "            predictions1[test_index] = q.predict(X1.loc[test_index])\n",
    "\n",
    "    assert np.isnan(predictions0).sum() == 0\n",
    "    assert np.isnan(predictions1).sum() == 0\n",
    "    return predictions0, predictions1\n",
    "\n",
    "\n",
    "def att_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "    \"\"\"\n",
    "    # Double ML estimator for the ATT\n",
    "    This uses the ATT specific scores, see equation 3.9 of https://www.econstor.eu/bitstream/10419/149795/1/869216953.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    if prob_t is None:\n",
    "        prob_t = A.mean() # estimate marginal probability of treatment\n",
    "\n",
    "    tau_hat = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0)).mean()/ prob_t\n",
    "  \n",
    "    scores = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0) - tau_hat*A) / prob_t\n",
    "    n = Y.shape[0] # number of observations\n",
    "    std_hat = np.std(scores) / np.sqrt(n)\n",
    "\n",
    "    return tau_hat, std_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jtf/anaconda3/envs/mamba-base-env/envs/nn_class/lib/python3.7/site-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate is -19.861800850414966 pm 11.89715227874221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-18.886275852614084"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf = pd.read_csv(\"data/wf.csv\")\n",
    "city_yb = pd.read_csv(\"data/city_yb.csv\")\n",
    "\n",
    "# wf.drop('Unnamed: 0', axis = 1)\n",
    "# city_yb.drop('Unnamed: 0', axis = 1)\n",
    "# city_yb = city_yb.dropna()\n",
    "# print(len(wf))\n",
    "# wf = wf.merge(city_yb, on='city_code').dropna(\n",
    "#     subset = ['sec_city', 'gdp_city', 'pgdp_city', \n",
    "#               'firm_city', 'gonglu', 'emit_ww', 'emit_so1', 'emi_dust1',\n",
    "#               'aqi', 'pm']\n",
    "# )\n",
    "# print(len(wf))\n",
    "\n",
    "wf[\"temp2\"] = wf[\"temp\"] ** 2\n",
    "wf[\"l_aqi\"] = np.log(1 + wf[\"aqi\"])\n",
    "wf[\"l_pm\"] = np.log(1 + wf[\"pm\"])\n",
    "wf2020 = wf[(wf[\"daynum\"] >= 8401) & (wf[\"daynum\"]<= 8461)].dropna(\n",
    "    subset = ['aqi', 'pm']\n",
    ")\n",
    "wf2020['cities'] = wf2020['city_code'].astype('category')\n",
    "wf2020['days'] = wf2020['daynum'].astype('category')\n",
    "wf2020 = pd.get_dummies(wf2020, drop_first=True)\n",
    "\n",
    "fixed = ['treat']\n",
    "city_fixed = []\n",
    "time_fixed = []\n",
    "for col in wf2020.columns:\n",
    "    if 'cities' in col:\n",
    "        city_fixed.append(col)\n",
    "    if 'days' in col:\n",
    "        time_fixed.append(col)\n",
    "fixed = fixed + city_fixed + time_fixed\n",
    "        \n",
    "weather = ['prec', 'snow', 'temp', 'temp2']\n",
    "city_economic = ['pop_city', 'sec_city', 'gdp_city' , 'pgdp_city', 'firm_city']\n",
    "city_environmental = ['gonglu', 'emit_ww', 'emit_so1', 'emi_dust1']\n",
    "out = [\"aqi\", \"l_aqi\", \"pm\", \"l_pm\"]\n",
    "\n",
    "treated = wf2020[wf2020['treat'] == 1]\n",
    "treated = treated[['daynum', 'city_code']].groupby('city_code')\n",
    "first = treated.apply(lambda x: x.sort_values(by = 'daynum', ascending=True).head(1))\n",
    "\n",
    "day, count = np.unique(first.daynum, return_counts = True)\n",
    "treat_day = day[count == max(count)][0]\n",
    "\n",
    "num_cities = {d:c for d,c in zip(day, count)}\n",
    "first = {city:day for day, city in first.values}\n",
    "\n",
    "wf2020 = wf2020.assign(first = [first.get(city, 0) for city in wf2020['city_code']])\n",
    "group = wf2020[(wf2020['first'] == treat_day) | (wf2020['first'] == 0)]\n",
    "wf2020['first'] = wf2020['first'].astype('category')\n",
    "wf2020 = pd.get_dummies(wf2020, drop_first=True)\n",
    "\n",
    "group['pre'] = group['daynum'] < treat_day\n",
    "group = group.groupby(['city_code', 'pre']).mean().reset_index('pre')\n",
    "\n",
    "compact = group[~group['pre']]\n",
    "aqi = group['aqi'].values\n",
    "compact['Y1-Y0'] = aqi[~group['pre']] - aqi[group['pre']]\n",
    "\n",
    "\n",
    "compact = compact.reset_index()\n",
    "outcome = compact['Y1-Y0']\n",
    "treatment = compact['treat']\n",
    "confounders = compact[city_fixed + time_fixed + weather]\n",
    "\n",
    "g = treatment_k_fold_fit_and_predict(make_g_model, X=confounders, A=treatment, n_splits=10)\n",
    "Q0,Q1=outcome_k_fold_fit_and_predict(make_Q_model, X=confounders, y=outcome, A=treatment, n_splits=10, output_type=\"continuous\")\n",
    "\n",
    "data_and_nuisance_estimates = pd.DataFrame({'g': g, 'Q0': Q0, 'Q1': Q1, 'A': treatment, 'Y': outcome})\n",
    "\n",
    "tau_hat, std_hat = att_aiptw(**data_and_nuisance_estimates)\n",
    "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")\n",
    "\n",
    "# for comparison, the point estimate without any covariate correction\n",
    "outcome[treatment==1].mean()-outcome[treatment==0].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_code</th>\n",
       "      <th>pre</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>prec</th>\n",
       "      <th>snow</th>\n",
       "      <th>temp</th>\n",
       "      <th>aqi</th>\n",
       "      <th>co</th>\n",
       "      <th>no2</th>\n",
       "      <th>...</th>\n",
       "      <th>days_8454</th>\n",
       "      <th>days_8455</th>\n",
       "      <th>days_8456</th>\n",
       "      <th>days_8457</th>\n",
       "      <th>days_8458</th>\n",
       "      <th>days_8459</th>\n",
       "      <th>days_8460</th>\n",
       "      <th>days_8461</th>\n",
       "      <th>first</th>\n",
       "      <th>Y1-Y0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3547</td>\n",
       "      <td>False</td>\n",
       "      <td>17973.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>12.066931</td>\n",
       "      <td>82.475749</td>\n",
       "      <td>-3.213173</td>\n",
       "      <td>63.077457</td>\n",
       "      <td>1.043483</td>\n",
       "      <td>29.020299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>8436.0</td>\n",
       "      <td>-47.483196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4870</td>\n",
       "      <td>False</td>\n",
       "      <td>41679.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>19.911129</td>\n",
       "      <td>60.060211</td>\n",
       "      <td>8.265350</td>\n",
       "      <td>52.420406</td>\n",
       "      <td>0.755353</td>\n",
       "      <td>24.584402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-38.480972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4956</td>\n",
       "      <td>False</td>\n",
       "      <td>45630.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>17.183897</td>\n",
       "      <td>55.427658</td>\n",
       "      <td>7.256715</td>\n",
       "      <td>74.777778</td>\n",
       "      <td>0.599033</td>\n",
       "      <td>15.702991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-67.323789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5996</td>\n",
       "      <td>False</td>\n",
       "      <td>42996.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>16.372821</td>\n",
       "      <td>60.033804</td>\n",
       "      <td>8.216290</td>\n",
       "      <td>53.088675</td>\n",
       "      <td>0.664156</td>\n",
       "      <td>20.947650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.060072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6145</td>\n",
       "      <td>False</td>\n",
       "      <td>4364.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>39.948305</td>\n",
       "      <td>58.197934</td>\n",
       "      <td>-1.215675</td>\n",
       "      <td>64.752137</td>\n",
       "      <td>1.073328</td>\n",
       "      <td>28.256410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.856384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>102298</td>\n",
       "      <td>False</td>\n",
       "      <td>118504.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>8.359323</td>\n",
       "      <td>48.622150</td>\n",
       "      <td>9.711950</td>\n",
       "      <td>35.453526</td>\n",
       "      <td>0.627885</td>\n",
       "      <td>6.076389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.763426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>102730</td>\n",
       "      <td>False</td>\n",
       "      <td>124211.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>7.468586</td>\n",
       "      <td>47.417817</td>\n",
       "      <td>6.140083</td>\n",
       "      <td>106.738782</td>\n",
       "      <td>1.157516</td>\n",
       "      <td>27.941773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-47.700253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>105012</td>\n",
       "      <td>False</td>\n",
       "      <td>104456.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>3.043103</td>\n",
       "      <td>48.781714</td>\n",
       "      <td>11.076449</td>\n",
       "      <td>78.638355</td>\n",
       "      <td>0.696608</td>\n",
       "      <td>19.128739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.184062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>105016</td>\n",
       "      <td>False</td>\n",
       "      <td>131235.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>4.517875</td>\n",
       "      <td>49.336072</td>\n",
       "      <td>0.334428</td>\n",
       "      <td>71.642094</td>\n",
       "      <td>0.685550</td>\n",
       "      <td>16.830662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.957029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>105229</td>\n",
       "      <td>False</td>\n",
       "      <td>138259.5</td>\n",
       "      <td>2.020022e+07</td>\n",
       "      <td>354.455191</td>\n",
       "      <td>82.875703</td>\n",
       "      <td>-6.289694</td>\n",
       "      <td>69.260149</td>\n",
       "      <td>0.990021</td>\n",
       "      <td>16.962073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-41.324694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     city_code    pre  Unnamed: 0          date        prec       snow  \\\n",
       "0         3547  False     17973.5  2.020022e+07   12.066931  82.475749   \n",
       "1         4870  False     41679.5  2.020022e+07   19.911129  60.060211   \n",
       "2         4956  False     45630.5  2.020022e+07   17.183897  55.427658   \n",
       "3         5996  False     42996.5  2.020022e+07   16.372821  60.033804   \n",
       "4         6145  False      4364.5  2.020022e+07   39.948305  58.197934   \n",
       "..         ...    ...         ...           ...         ...        ...   \n",
       "250     102298  False    118504.5  2.020022e+07    8.359323  48.622150   \n",
       "251     102730  False    124211.5  2.020022e+07    7.468586  47.417817   \n",
       "252     105012  False    104456.5  2.020022e+07    3.043103  48.781714   \n",
       "253     105016  False    131235.5  2.020022e+07    4.517875  49.336072   \n",
       "254     105229  False    138259.5  2.020022e+07  354.455191  82.875703   \n",
       "\n",
       "          temp         aqi        co        no2  ...  days_8454  days_8455  \\\n",
       "0    -3.213173   63.077457  1.043483  29.020299  ...   0.038462   0.038462   \n",
       "1     8.265350   52.420406  0.755353  24.584402  ...   0.038462   0.038462   \n",
       "2     7.256715   74.777778  0.599033  15.702991  ...   0.038462   0.038462   \n",
       "3     8.216290   53.088675  0.664156  20.947650  ...   0.038462   0.038462   \n",
       "4    -1.215675   64.752137  1.073328  28.256410  ...   0.038462   0.038462   \n",
       "..         ...         ...       ...        ...  ...        ...        ...   \n",
       "250   9.711950   35.453526  0.627885   6.076389  ...   0.038462   0.038462   \n",
       "251   6.140083  106.738782  1.157516  27.941773  ...   0.038462   0.038462   \n",
       "252  11.076449   78.638355  0.696608  19.128739  ...   0.038462   0.038462   \n",
       "253   0.334428   71.642094  0.685550  16.830662  ...   0.038462   0.038462   \n",
       "254  -6.289694   69.260149  0.990021  16.962073  ...   0.038462   0.038462   \n",
       "\n",
       "     days_8456  days_8457  days_8458  days_8459  days_8460  days_8461   first  \\\n",
       "0     0.038462   0.038462   0.038462   0.038462   0.038462   0.038462  8436.0   \n",
       "1     0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "2     0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "3     0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "4     0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "..         ...        ...        ...        ...        ...        ...     ...   \n",
       "250   0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "251   0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "252   0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "253   0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "254   0.038462   0.038462   0.038462   0.038462   0.038462   0.038462     0.0   \n",
       "\n",
       "         Y1-Y0  \n",
       "0   -47.483196  \n",
       "1   -38.480972  \n",
       "2   -67.323789  \n",
       "3   -37.060072  \n",
       "4    -1.856384  \n",
       "..         ...  \n",
       "250   6.763426  \n",
       "251 -47.700253  \n",
       "252   3.184062  \n",
       "253  -7.957029  \n",
       "254 -41.324694  \n",
       "\n",
       "[255 rows x 410 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.684548868980686\n",
      "-11.746 pm 1.545\n"
     ]
    }
   ],
   "source": [
    "wf2020 = wf2020.assign(first = [first.get(city, 0) for city in wf2020['city_code']])\n",
    "wf2020['A'] = (wf2020['daynum'] == wf2020['first']).astype('int64')\n",
    "\n",
    "wf2020['diff'] = wf2020.sort_values(by = 'daynum')['aqi'] \\\n",
    "            - wf2020.sort_values(by = 'daynum').groupby('city_code')['aqi'].shift(1)\n",
    "wf2020 = wf2020.dropna(subset= ['diff'])\n",
    "\n",
    "outcome = wf2020['diff']\n",
    "treatment = wf2020['A']\n",
    "confounders = wf2020[['daynum'] + weather + city_fixed + time_fixed]\n",
    "\n",
    "# specify a model for the conditional expected outcome\n",
    "\n",
    "# TODO(victorveitch) the covariates have basically no predictive power, replace this example with something better\n",
    "\n",
    "# make a function that returns a sklearn model for later use in k-folding\n",
    "def make_Q_model():\n",
    "    #return LinearRegression()\n",
    "    return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators = 100, max_depth=10)\n",
    "Q_model = make_Q_model()\n",
    "\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_w_treatment = confounders.copy()\n",
    "X_w_treatment[\"treatment\"] = treatment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w_treatment, outcome, test_size=0.2)\n",
    "Q_model.fit(X_train, y_train)\n",
    "y_pred = Q_model.predict(X_test)\n",
    "\n",
    "\n",
    "# specify a model for the propensity score\n",
    "\n",
    "def make_g_model():\n",
    "    #return LogisticRegression(max_iter=1000)\n",
    "    return RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=100, max_depth=3)\n",
    "\n",
    "g_model = make_g_model()\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_train, X_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.2)\n",
    "g_model.fit(X_train, a_train)\n",
    "a_pred = g_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# for comparison, the point estimate without any covariate correction\n",
    "print(outcome[treatment==1].mean()-outcome[treatment==0].mean())\n",
    "\n",
    "res = dict()\n",
    "Q_model.fit(X_w_treatment, outcome)\n",
    "g_model.fit(confounders, treatment)\n",
    "\n",
    "for day in np.unique(wf2020['daynum']):\n",
    "    df = wf2020[wf2020['daynum'] == day]\n",
    "    outcome_t = df['diff']\n",
    "    treatment_t = df['A']\n",
    "    confounders_t = df[['daynum'] + weather + city_fixed + time_fixed]\n",
    "    \n",
    "    if df['A'].sum() == 0 or num_cities[day] < 2:\n",
    "        continue\n",
    "    \n",
    "    X1 = confounders_t.copy()\n",
    "    X0 = confounders_t.copy()\n",
    "    X1[\"treatment\"] = 1\n",
    "    X0[\"treatment\"] = 0\n",
    "    \n",
    "    Q0 = Q_model.predict(X0)\n",
    "    Q1 = Q_model.predict(X1)\n",
    "    g = g_model.predict_proba(confounders_t)[:,1]\n",
    "    \n",
    "    est, sd = att_aiptw(Q0, Q1, g, treatment_t, outcome_t)\n",
    "    res[day] = (est, sd)\n",
    "    \n",
    "inv_var = np.array([1/v**2 for p,v in res.values()])\n",
    "point = np.array([p for p,v in res.values()])    \n",
    "\n",
    "tau_hat = (point * inv_var).sum()/inv_var.sum()\n",
    "std_hat = np.sqrt(1/inv_var.sum())\n",
    "\n",
    "print('%0.3f pm %0.3f' % (tau_hat, std_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric multiple-time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jtf/anaconda3/envs/mamba-base-env/envs/nn_class/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# create week coefficient \n",
    "treated = wf2020[wf2020['treat'] == 1]\n",
    "treated = treated[['daynum', 'city_code']].groupby('city_code')\n",
    "first = treated.apply(lambda x: x.sort_values(by = 'daynum', ascending=True).head(1))\n",
    "day, count = np.unique(first.daynum, return_counts = True)\n",
    "treat_day = day[count == max(count)][0]\n",
    "first = {city:day for day, city in first.values}\n",
    "wf2020 = wf2020.assign(first = [first.get(city, 0) for city in wf2020['city_code']])\n",
    "wf2020[\"week_coef\"] = np.floor((wf2020[\"daynum\"] - wf2020[\"first\"])/7).astype(int)\n",
    "\n",
    "# set -1 lead and untreated to NaN so they don't get week0 dummy\n",
    "wf2020[\"week_coef\"] = np.where((wf2020[\"week_coef\"] == -1), np.NaN, wf2020[\"week_coef\"])\n",
    "wf2020[\"week_coef\"][wf2020[\"first\"] == 0] = np.NaN\n",
    "wf2020[\"week_coef\"] = wf2020[\"week_coef\"].astype('category')\n",
    "wf2020 = pd.get_dummies(wf2020)\n",
    "\n",
    "week_coef = []\n",
    "for col in wf2020.columns:\n",
    "    if 'week_coef' in col:\n",
    "        week_coef.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aqi\n",
      "('week_coef_-8.0', -19.67750644058875, 34.78439824125761)\n",
      "('week_coef_-7.0', -7.5558915665224, 30.006947920031873)\n",
      "('week_coef_-6.0', -5.2049938273005125, 17.09890057195802)\n",
      "('week_coef_-5.0', 0.671156619812766, 5.340276359861647)\n",
      "('week_coef_-4.0', -6.409588715384092, 4.698786186081183)\n",
      "('week_coef_-3.0', -2.9318971044050044, 4.299834886919208)\n",
      "('week_coef_-2.0', 2.587539405094274, 4.244627961084665)\n",
      "('week_coef_0.0', 11.059228639271826, 3.3256560165105364)\n",
      "('week_coef_1.0', 6.649594635204274, 3.3192584528342475)\n",
      "('week_coef_2.0', -6.443418794640473, 3.271662615242559)\n",
      "('week_coef_3.0', -8.533294552083818, 3.425156780194195)\n",
      "('week_coef_4.0', -9.942218345641072, 4.878733474988247)\n",
      "('week_coef_5.0', -15.482991974087353, 9.309041106773398)\n",
      "l_aqi\n",
      "('week_coef_-8.0', -0.17094816949989425, 0.3640543449416856)\n",
      "('week_coef_-7.0', -0.09276540879467923, 0.31405343547870795)\n",
      "('week_coef_-6.0', -0.007958147742662293, 0.1789575028371148)\n",
      "('week_coef_-5.0', 0.0003076119666239954, 0.055891460260797415)\n",
      "('week_coef_-4.0', -0.055725794390792005, 0.04917760874086002)\n",
      "('week_coef_-3.0', -0.035506995369796956, 0.04500217489052597)\n",
      "('week_coef_-2.0', -0.003100732636886252, 0.044424377882754246)\n",
      "('week_coef_0.0', 0.12781200925697894, 0.03480639550510004)\n",
      "('week_coef_1.0', 0.08640486831181038, 0.034739438450467675)\n",
      "('week_coef_2.0', -0.0612184093794384, 0.034241299274501105)\n",
      "('week_coef_3.0', -0.07230887676365495, 0.035847772880462744)\n",
      "('week_coef_4.0', -0.07016452106185816, 0.0510609413755868)\n",
      "('week_coef_5.0', -0.2329219623361655, 0.09742864713818564)\n",
      "pm\n",
      "('week_coef_-8.0', -17.632896307292224, 26.80723047966142)\n",
      "('week_coef_-7.0', -11.48805329311245, 23.12540131654175)\n",
      "('week_coef_-6.0', -9.186610918128025, 13.177579367683805)\n",
      "('week_coef_-5.0', -0.7085067364847483, 4.115581307774342)\n",
      "('week_coef_-4.0', -8.435497207311574, 3.6212052136502177)\n",
      "('week_coef_-3.0', -1.9125490005293848, 3.313746123726667)\n",
      "('week_coef_-2.0', 1.5761009084149433, 3.2711999001394267)\n",
      "('week_coef_0.0', 6.485717135962499, 2.5629774220135366)\n",
      "('week_coef_1.0', 5.301255201558019, 2.5580470229654035)\n",
      "('week_coef_2.0', -3.958136339948399, 2.52136642325103)\n",
      "('week_coef_3.0', -6.740813854617227, 2.6396594990318034)\n",
      "('week_coef_4.0', -7.209223449966306, 3.7598848715377673)\n",
      "('week_coef_5.0', -10.80771045898315, 7.174182194071344)\n",
      "l_pm\n",
      "('week_coef_-8.0', -0.19564266674422426, 0.41908751972802927)\n",
      "('week_coef_-7.0', -0.1652868391250783, 0.3615280992125521)\n",
      "('week_coef_-6.0', -0.050506315481156513, 0.20601005603364397)\n",
      "('week_coef_-5.0', -0.01165740358959129, 0.06434043098270746)\n",
      "('week_coef_-4.0', -0.07659996682514635, 0.056611663504974104)\n",
      "('week_coef_-3.0', -0.02729379854155156, 0.05180503987738013)\n",
      "('week_coef_-2.0', -0.018427152836803636, 0.051139898756946266)\n",
      "('week_coef_0.0', 0.11144376394883879, 0.04006799030304595)\n",
      "('week_coef_1.0', 0.08325280414089839, 0.039990911519770696)\n",
      "('week_coef_2.0', -0.04616559866282138, 0.03941746990415522)\n",
      "('week_coef_3.0', -0.07740352670799286, 0.041266790063042076)\n",
      "('week_coef_4.0', -0.05799344842723996, 0.05877969477194588)\n",
      "('week_coef_5.0', -0.23787586449287074, 0.11215668937048376)\n"
     ]
    }
   ],
   "source": [
    "for Yname in out:\n",
    "    Y = wf2020[Yname]\n",
    "    X = wf2020[fixed + weather + week_coef]\n",
    "    fit = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "    print(Yname)\n",
    "    print(*list(zip([index for index in fit.params.index if 'week_coef' in index],\n",
    "                    fit.params[[index for index in fit.params.index if 'week_coef' in index]], \n",
    "                   2*fit.bse[[index for index in fit.params.index if 'week_coef' in index]])), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check parallel trends on two-period subsets (23 and 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8402, 0.0),\n",
       " (8403, 0.0),\n",
       " (8404, 0.0),\n",
       " (8405, 0.0),\n",
       " (8406, 0.0),\n",
       " (8407, 0.0),\n",
       " (8408, 0.0),\n",
       " (8409, 0.0),\n",
       " (8410, 0.0),\n",
       " (8411, 0.0),\n",
       " (8412, 0.0),\n",
       " (8413, 0.0),\n",
       " (8414, 0.0),\n",
       " (8415, 0.0),\n",
       " (8416, 0.0),\n",
       " (8417, 0.0),\n",
       " (8418, 0.0),\n",
       " (8419, 0.0),\n",
       " (8420, 0.0),\n",
       " (8421, 0.0),\n",
       " (8422, 0.0),\n",
       " (8423, 0.0),\n",
       " (8424, -0.38133389606799645),\n",
       " (8425, -0.126985867358723),\n",
       " (8426, 1.790454412458783),\n",
       " (8427, 0.0),\n",
       " (8428, 0.0),\n",
       " (8429, 0.0),\n",
       " (8430, 0.0),\n",
       " (8431, 0.0),\n",
       " (8432, 0.0),\n",
       " (8433, 0.0),\n",
       " (8434, 0.0),\n",
       " (8435, 0.0),\n",
       " (8436, 0.0),\n",
       " (8437, 0.0),\n",
       " (8438, 0.0),\n",
       " (8439, 0.0),\n",
       " (8440, 0.0),\n",
       " (8441, 0.0),\n",
       " (8442, 0.0),\n",
       " (8443, 0.0),\n",
       " (8444, 0.0),\n",
       " (8445, 0.0),\n",
       " (8446, 0.0),\n",
       " (8447, 0.0),\n",
       " (8448, 0.0),\n",
       " (8449, 0.0),\n",
       " (8450, 0.0),\n",
       " (8451, 0.0),\n",
       " (8452, 0.0),\n",
       " (8453, 0.0),\n",
       " (8454, 0.0),\n",
       " (8455, 0.0),\n",
       " (8456, 0.0),\n",
       " (8457, 0.0),\n",
       " (8458, 0.0),\n",
       " (8459, 0.0),\n",
       " (8460, 0.0),\n",
       " (8461, 0.0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E[Y_{t+1} - Y_{t} | A=1, X] - E[Y_{t+1} - Y_{t} | A=0, X] = 0 in all pre-treatment periods t.\n",
    "#  if you've fit a model for Q(a,x) = E[Y_{t+1} - Y_{t} | A=a, x] then you can plot diff(t) = 1/n \\sum_i Q(1,x_i) - Q(0,x_i)\n",
    "\n",
    "\n",
    "\n",
    "day_23 = day[count == max(count)][0]\n",
    "wf23 = wf2020[(wf2020['first'] == day_23) | (wf2020['first'] == 0)]\n",
    "\n",
    "# estimate for each day\n",
    "day_diffs = []\n",
    "day_list = np.sort(np.unique(wf23['daynum']))\n",
    "for d in day_list:\n",
    "    df = wf23[wf23['daynum'] == d]\n",
    "    confounders_t = df[['daynum'] + weather + city_fixed + time_fixed]\n",
    "    \n",
    "    X1 = confounders_t.copy()\n",
    "    X0 = confounders_t.copy()\n",
    "    X1[\"treatment\"] = 1\n",
    "    X0[\"treatment\"] = 0\n",
    "    \n",
    "    Q0 = Q_model.predict(X0)\n",
    "    Q1 = Q_model.predict(X1)\n",
    "    \n",
    "    day_diffs.append(np.mean(Q1 - Q0))\n",
    "\n",
    "list(zip(day_list, day_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daynum</th>\n",
       "      <th>prec</th>\n",
       "      <th>snow</th>\n",
       "      <th>temp</th>\n",
       "      <th>temp2</th>\n",
       "      <th>cities_3547</th>\n",
       "      <th>cities_4870</th>\n",
       "      <th>cities_4956</th>\n",
       "      <th>cities_5993</th>\n",
       "      <th>cities_5996</th>\n",
       "      <th>...</th>\n",
       "      <th>days_8453</th>\n",
       "      <th>days_8454</th>\n",
       "      <th>days_8455</th>\n",
       "      <th>days_8456</th>\n",
       "      <th>days_8457</th>\n",
       "      <th>days_8458</th>\n",
       "      <th>days_8459</th>\n",
       "      <th>days_8460</th>\n",
       "      <th>days_8461</th>\n",
       "      <th>treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16225</th>\n",
       "      <td>8457</td>\n",
       "      <td>0.093731</td>\n",
       "      <td>123.399300</td>\n",
       "      <td>-1.931199</td>\n",
       "      <td>3.729528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16226</th>\n",
       "      <td>8458</td>\n",
       "      <td>0.417873</td>\n",
       "      <td>118.744040</td>\n",
       "      <td>-1.312069</td>\n",
       "      <td>1.721525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16227</th>\n",
       "      <td>8459</td>\n",
       "      <td>0.769849</td>\n",
       "      <td>63.414462</td>\n",
       "      <td>0.400970</td>\n",
       "      <td>0.160777</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16228</th>\n",
       "      <td>8460</td>\n",
       "      <td>0.335493</td>\n",
       "      <td>41.602661</td>\n",
       "      <td>0.971140</td>\n",
       "      <td>0.943113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16229</th>\n",
       "      <td>8461</td>\n",
       "      <td>0.146075</td>\n",
       "      <td>103.232866</td>\n",
       "      <td>-0.591814</td>\n",
       "      <td>0.350244</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113244</th>\n",
       "      <td>8457</td>\n",
       "      <td>0.050583</td>\n",
       "      <td>63.074559</td>\n",
       "      <td>13.997334</td>\n",
       "      <td>195.925373</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113245</th>\n",
       "      <td>8458</td>\n",
       "      <td>73.515920</td>\n",
       "      <td>12.815312</td>\n",
       "      <td>14.244464</td>\n",
       "      <td>202.904752</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113246</th>\n",
       "      <td>8459</td>\n",
       "      <td>83.192755</td>\n",
       "      <td>39.559249</td>\n",
       "      <td>14.188429</td>\n",
       "      <td>201.311514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113247</th>\n",
       "      <td>8460</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>35.937346</td>\n",
       "      <td>13.350782</td>\n",
       "      <td>178.243391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113248</th>\n",
       "      <td>8461</td>\n",
       "      <td>0.029087</td>\n",
       "      <td>64.019735</td>\n",
       "      <td>13.848076</td>\n",
       "      <td>191.769205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        daynum       prec        snow       temp       temp2  cities_3547  \\\n",
       "16225     8457   0.093731  123.399300  -1.931199    3.729528            0   \n",
       "16226     8458   0.417873  118.744040  -1.312069    1.721525            0   \n",
       "16227     8459   0.769849   63.414462   0.400970    0.160777            0   \n",
       "16228     8460   0.335493   41.602661   0.971140    0.943113            0   \n",
       "16229     8461   0.146075  103.232866  -0.591814    0.350244            0   \n",
       "...        ...        ...         ...        ...         ...          ...   \n",
       "113244    8457   0.050583   63.074559  13.997334  195.925373            0   \n",
       "113245    8458  73.515920   12.815312  14.244464  202.904752            0   \n",
       "113246    8459  83.192755   39.559249  14.188429  201.311514            0   \n",
       "113247    8460   0.097458   35.937346  13.350782  178.243391            0   \n",
       "113248    8461   0.029087   64.019735  13.848076  191.769205            0   \n",
       "\n",
       "        cities_4870  cities_4956  cities_5993  cities_5996  ...  days_8453  \\\n",
       "16225             0            0            0            0  ...          0   \n",
       "16226             0            0            0            0  ...          0   \n",
       "16227             0            0            0            0  ...          0   \n",
       "16228             0            0            0            0  ...          0   \n",
       "16229             0            0            0            0  ...          0   \n",
       "...             ...          ...          ...          ...  ...        ...   \n",
       "113244            0            0            0            0  ...          0   \n",
       "113245            0            0            0            0  ...          0   \n",
       "113246            0            0            0            0  ...          0   \n",
       "113247            0            0            0            0  ...          0   \n",
       "113248            0            0            0            0  ...          0   \n",
       "\n",
       "        days_8454  days_8455  days_8456  days_8457  days_8458  days_8459  \\\n",
       "16225           0          0          0          1          0          0   \n",
       "16226           0          0          0          0          1          0   \n",
       "16227           0          0          0          0          0          1   \n",
       "16228           0          0          0          0          0          0   \n",
       "16229           0          0          0          0          0          0   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "113244          0          0          0          1          0          0   \n",
       "113245          0          0          0          0          1          0   \n",
       "113246          0          0          0          0          0          1   \n",
       "113247          0          0          0          0          0          0   \n",
       "113248          0          0          0          0          0          0   \n",
       "\n",
       "        days_8460  days_8461  treatment  \n",
       "16225           0          0          1  \n",
       "16226           0          0          1  \n",
       "16227           0          0          1  \n",
       "16228           1          0          1  \n",
       "16229           0          1          1  \n",
       "...           ...        ...        ...  \n",
       "113244          0          0          1  \n",
       "113245          0          0          1  \n",
       "113246          0          0          1  \n",
       "113247          1          0          1  \n",
       "113248          0          1          1  \n",
       "\n",
       "[115 rows x 389 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('week_coef_-5.0', 0.0),\n",
       " ('week_coef_-4.0', 0.0),\n",
       " ('week_coef_-3.0', 0.0),\n",
       " ('week_coef_-2.0', -0.05448249179145666),\n",
       " ('week_coef_0.0', 0.0),\n",
       " ('week_coef_1.0', 0.0),\n",
       " ('week_coef_2.0', 0.0),\n",
       " ('week_coef_3.0', 0.0)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate by week\n",
    "weeks_list = []\n",
    "week_diffs = []\n",
    "for w in week_coef:\n",
    "    wf23_week = wf23[wf23[w] == 1]\n",
    "    if len(wf23_week) == 0:\n",
    "        continue\n",
    "    confounders_t = wf23_week[['daynum'] + weather + city_fixed + time_fixed]\n",
    "    \n",
    "    X1 = confounders_t.copy()\n",
    "    X0 = confounders_t.copy()\n",
    "    X1[\"treatment\"] = 1\n",
    "    X0[\"treatment\"] = 0\n",
    "    \n",
    "    Q0 = Q_model.predict(X0)\n",
    "    Q1 = Q_model.predict(X1)\n",
    "    weeks_list.append(w)\n",
    "    week_diffs.append(np.mean(Q1 - Q0))\n",
    "    \n",
    "list(zip(weeks_list, week_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_16 = day[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariate Influence Strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_austen_format(nuisance_estimate_df: pd.DataFrame):\n",
    "  austen_df = pd.DataFrame()\n",
    "  austen_df['y']=nuisance_estimate_df['Y']\n",
    "  austen_df['t']=nuisance_estimate_df['A']\n",
    "  austen_df['g']=nuisance_estimate_df['g']\n",
    "  A = nuisance_estimate_df['A']\n",
    "  austen_df['Q']=A*nuisance_estimate_df['Q1'] + (1-A)*nuisance_estimate_df['Q0'] # use Q1 when A=1, and Q0 when A=0\n",
    "\n",
    "  return austen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_groups = {\n",
    "    'city_fixed': city_fixed,\n",
    "    'time_fixed': time_fixed,\n",
    "    'weather': weather,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each covariate group, refit the models without using that group\n",
    "nuisance_estimates = {}\n",
    "for group, covs in covariate_groups.items():\n",
    "    remaining_confounders = confounders.drop(columns=covs)\n",
    "    print(remaining_confounders)\n",
    "    \n",
    "    g = treatment_k_fold_fit_and_predict(make_g_model, X=remaining_confounders, A=treatment, n_splits=10)\n",
    "    Q0, Q1 = outcome_k_fold_fit_and_predict(make_Q_model, X=remaining_confounders, y=outcome, A=treatment, n_splits=10, output_type=\"continuous\")\n",
    "\n",
    "    data_and_nuisance_estimates = pd.DataFrame({'g': g, 'Q0': Q0, 'Q1': Q1, 'A': treatment, 'Y': outcome})\n",
    "    nuisance_estimates[group] = data_and_nuisance_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_nuisance_path = 'data_and_nuisance_estimates.csv'\n",
    "covariate_dir_path = 'covariates/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_austen_format(nuisance_estimate_df: pd.DataFrame):\n",
    "  austen_df = pd.DataFrame()\n",
    "  austen_df['y']=nuisance_estimate_df['Y']\n",
    "  austen_df['t']=nuisance_estimate_df['A']\n",
    "  austen_df['g']=nuisance_estimate_df['g']\n",
    "  A = nuisance_estimate_df['A']\n",
    "  austen_df['Q']=A*nuisance_estimate_df['Q1'] + (1-A)*nuisance_estimate_df['Q0'] # use Q1 when A=1, and Q0 when A=0\n",
    "\n",
    "  return austen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_data_and_nuisance = _convert_to_austen_format(data_and_nuisance_estimates)\n",
    "austen_data_and_nuisance.to_csv(data_and_nuisance_path, index=False)\n",
    "\n",
    "pathlib.Path(covariate_dir_path).mkdir(exist_ok=True)\n",
    "for group, nuisance_estimate in nuisance_estimates.items():\n",
    "  austen_nuisance_estimate = _convert_to_austen_format(nuisance_estimate)\n",
    "  austen_nuisance_estimate.to_csv(os.path.join(covariate_dir_path,group+\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bias = 8.00 # note: bias is specified as an absolute number\n",
    "ap = AustenPlot(data_and_nuisance_path, covariate_dir_path)\n",
    "p, plot_coords, variable_coords = ap.fit(bias=target_bias)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each covariate group, refit the models without using that group\n",
    "nuisance_estimates = {}\n",
    "for group, covs in covariate_groups.items():\n",
    "    # make model with covariates dropped\n",
    "    Q_model = make_Q_model()\n",
    "    g_model = make_g_model()\n",
    "    \n",
    "    remaining_confounders = confounders.drop(columns=covs)\n",
    "    X_w_treatment_rem = remaining_confounders.copy()\n",
    "    X_w_treatment_rem[\"treatment\"] = treatment\n",
    "    Q_model.fit(X_w_treatment_rem, outcome)\n",
    "    g_model.fit(remaining_confounders, treatment)\n",
    "    \n",
    "    g_list = []\n",
    "    Q0_list = []\n",
    "    Q1_list = []\n",
    "    t_list = []\n",
    "    out_list = []\n",
    "    for day in np.unique(wf2020['daynum']):\n",
    "        df = wf2020[wf2020['daynum'] == day]\n",
    "        outcome_t = df['diff']\n",
    "        treatment_t = df['A']\n",
    "        confounders_t = df[['daynum'] + weather + city_fixed + time_fixed]\n",
    "        remaining_confounders_t = confounders_t.drop(columns=covs)\n",
    "\n",
    "        if df['A'].sum() == 0 or num_cities[day] < 2:\n",
    "            continue\n",
    "\n",
    "        X1 = remaining_confounders_t.copy()\n",
    "        X0 = remaining_confounders_t.copy()\n",
    "        X1[\"treatment\"] = 1\n",
    "        X0[\"treatment\"] = 0\n",
    "\n",
    "        Q0 = Q_model.predict(X0)\n",
    "        Q1 = Q_model.predict(X1)\n",
    "        g = g_model.predict_proba(remaining_confounders_t)[:,1]\n",
    "        \n",
    "        g_list += list(g)\n",
    "        Q0_list += list(Q0)\n",
    "        Q1_list += list(Q1)\n",
    "        t_list += list(treatment_t)\n",
    "        out_list += list(outcome_t)\n",
    "\n",
    "#         est, sd = att_aiptw(Q0, Q1, g, treatment_t, outcome_t)\n",
    "#         res[day] = (est, sd)\n",
    "    \n",
    "#     inv_var = np.array([1/v**2 for p,v in res.values()])\n",
    "#     point = np.array([p for p,v in res.values()])\n",
    "    \n",
    "#     tau_hat = (point * inv_var).sum()/inv_var.sum()\n",
    "#     std_hat = np.sqrt(1/inv_var.sum())\n",
    "    \n",
    "    data_and_nuisance_estimates = pd.DataFrame({'g': pd.Series(g_list), \n",
    "                                                'Q0': pd.Series(Q0_list), 'Q1': pd.Series(Q1_list), \n",
    "                                                'A': pd.Series(t_list), 'Y': pd.Series(out_list)})\n",
    "    nuisance_estimates[group] = data_and_nuisance_estimates\n",
    "\n",
    "austen_data_and_nuisance = _convert_to_austen_format(data_and_nuisance_estimates)\n",
    "austen_data_and_nuisance.to_csv(data_and_nuisance_path, index=False)\n",
    "\n",
    "pathlib.Path(covariate_dir_path).mkdir(exist_ok=True)\n",
    "for group, nuisance_estimate in nuisance_estimates.items():\n",
    "  austen_nuisance_estimate = _convert_to_austen_format(nuisance_estimate)\n",
    "  austen_nuisance_estimate.to_csv(os.path.join(covariate_dir_path,group+\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_data_and_nuisance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bias = 5.00 # note: bias is specified as an absolute number\n",
    "ap = AustenPlot(data_and_nuisance_path, covariate_dir_path)\n",
    "p, plot_coords, variable_coords = ap.fit(bias=target_bias)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_class",
   "language": "python",
   "name": "nn_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
